
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PrimDiffusion</title>

    <meta name="description" content="Project page for PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation.">
    <meta name="viewport" content="width=device-width, initial-scale=1">

        <!--FACEBOOK-->
    <meta property="og:image" content="https://frozenburning.github.io/projects/primdiffusion/img/teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://frozenburning.github.io/projects/primdiffusion"/>
    <meta property="og:title" content="PrimDiffusion" />
    <meta property="og:description" content="Project page for PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="PrimDiffusion" />
    <meta name="twitter:description" content="Project page for PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation." />
    <meta name="twitter:image" content="https://frozenburning.github.io/projects/primdiffusion/img/teaser.png" />

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <!-- <link rel="stylesheet" href="css/app.css"> -->
    <link rel="stylesheet" href="../../stylesheet.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main" style="width: 100%; max-width: 1500px;">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>PrimDiffusion</b>: Volumetric Primitives Diffusion for 3D Human Generation<br>
                <small>
                    NeurIPS 2023
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://frozenburning.github.io">
                            Zhaoxi Chen<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://hongfz16.github.io/">
                            Fangzhou Hong<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=TOZ9wR4AAAAJ&hl=en">
                            Haiyi Mei<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://wanggcong.github.io/">
                            Guangcong Wang<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en">
                            Lei Yang<sup>2</sup>
                        </a>
                    </li>
                    <li>
                        <a href="http://liuziwei7.github.io">
                            Ziwei Liu<sup>1</sup>
                        </a>
                    </li>
                    </br><sup>1</sup>S-Lab, Nanyang Technological University &emsp; <sup>2</sup>Sensetime Research
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2312.04559">
                            <image src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5cab99df4998decfbf9e218e_paper-01-p-500.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/zprHGZ7Gm7A">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/FrozenBurning/PrimDiffusion">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    PrimDiffusion performs the diffusion and denoising process on a set of primitives which compactly represent 3D humans. This generative modeling enables explicit pose, view, and shape control, with the capability of modeling off-body topology in well-defined depth. Moreover, our method can generalize to novel poses without post-processing and enable downstream human-centric tasks like 3D texture transfer.
                </p>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/teaser.mp4" type="video/mp4" />
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Framework
                </h3>
                <p class="text-justify">
                    We represent the 3D human as K primitives learned from multi-view images. Each primitive V<sub>k</sub> has independent kinematic parameters {T<sub>k</sub>, R<sub>k</sub>, s<sub>k</sub>} (translation, rotation, and per-axis scales, respectively) and radiance parameters {c<sub>k</sub>, σ<sub>k</sub>} (color and density). For each time step t, we diffuse the primitives V<sub>0</sub> with noise ϵ sampled according to a fixed noise schedule. The resulting V<sub>t</sub> is fed to g<sub>Φ</sub>(·) which learns to predict the denoised volumetric primitives.                  
                </p>
                <p style="text-align:center;">
                    <image src="img/framework.png" class="img-responsive" alt="scales">
                </p>
                <br>
                <p class="text-justify">
                    To get rid of per-subject optimization, we propose an encoder-only network that is capable of learning primitives from multi-view images across identities. The encoder consists of a motion branch and an appearance branch, which are fused by the proposed cross-modal attention layer to get kinematic and radiance information of primitives.        
                </p>
                <p style="text-align:center;">
                    <image src="img/prim_fit.png" class="img-responsive" alt="scales">
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visualization of the Denoising Process
                </h3>
                <p class="text-justify">
                    We visualize the denoising process of primitives and corresponding 360-degree novel views.
                </p>
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/denoising_novelview.mp4" type="video/mp4" />
                </video>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/zprHGZ7Gm7A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10" style="padding: 0%; margin: 0%;">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{chen2023primdiffusion,
    title={PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation},
    author={Zhaoxi Chen and Fangzhou Hong and Haiyi Mei and Guangcong Wang and Lei Yang and Ziwei Liu},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    PrimDiffusion is implemented on top of the <a href="https://github.com/facebookresearch/dva">DVA</a> and <a href="https://github.com/CompVis/latent-diffusion"> Latent-Diffusion</a> codebase. The training data are rendered via <a href="https://github.com/openxrlab/xrfeitoria">XRFeitoria</a> toolchain.
                    <br>
                The website template is borrowed from <a href="https://jonbarron.info/mipnerf/">Mip-NeRF</a>.
                </p>
                <br>
            </div>
        </div>
    </div>
</body>
</html>
