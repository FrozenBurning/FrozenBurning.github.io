<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Zhaoxi Chen</title>
  
  <meta name="author" content="Zhaoxi Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhaoxi Chen</name>
              </p>
              <p>I'm Zhaoxi Chen, a second-year Ph.D. student in the School of Computer Science and Engineering at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a> (<a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>), supervised by Prof. <a href="https://liuziwei7.github.io/">Ziwei Liu</a>. Prior to it, I received my bachelor degree in the <a href="https://www.au.tsinghua.edu.cn/">Department of Automation</a> from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021.
              </p>
              <p style="text-align:center">
                <a href="mailto:zhaoxi001@ntu.edu.sg">Email</a> &nbsp/&nbsp
                <a href="./data/resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=HsV0WbwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/FrozenBurning/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/me.JPG" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                [04/2023] Selected as a Finalist of <a href="https://research.facebook.com/blog/2023/4/announcing-the-2023-meta-research-phd-fellowship-award-winners/">2023 Meta Research PhD Fellowship</a>!
              </p>
              <p>
                [02/2023] One paper accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a> as <font color="#ff6a5c"><strong>Highlight</strong></font>.
              </p>
              <p>
                [01/2023] One paper accepted to <a href="https://iclr.cc/Conferences/2023">ICLR 2023</a> as <font color="#ff6a5c"><strong>Spotlight</strong></font>.
              </p>
              <p>
                [08/2022] One paper accepted to <a href="https://dl.acm.org/journal/tog">TOG</a> (Proc. <a href="https://sa2022.siggraph.org/en/"> SIGGRAPH Asia 2022</a>).
              </p>
              <p>
                [07/2022] One paper accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </p>
              <p>
                [08/2021] Join <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>!
              </p>
              <p>
                [07/2021] One paper accepted to <a href="https://iccv2021.thecvf.com/home">ICCV 2021</a> for <font color="#ff6a5c"><strong>Oral</strong></font> presentation.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                I'm interested in computer vision and deep learning, especially in the field of neural rendering. <br>
                <strong>*</strong> denotes equal contributions.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/scenedreamer_small.gif' width="200">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="projects/scenedreamer/">
                <papertitle>SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections</papertitle>
              </a>
              <br>
              <strong>Zhaoxi Chen</strong>,
              <a href="https://wanggcong.github.io/">Guangcong Wang</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              [<a href="https://scene-dreamer.github.io/">Project Page</a>]
							[<a href="https://arxiv.org/abs/2302.01330">Paper</a>]
							[<a href="https://youtu.be/nEfSKL2_FoA">Video</a>]
							[<a href="https://github.com/FrozenBurning/SceneDreamer">Code</a>]
              <img src="https://img.shields.io/github/stars/FrozenBurning/SceneDreamer?style=social">
              <img src="https://img.shields.io/github/forks/FrozenBurning/SceneDreamer?style=social">
              <p></p>
              <p>SceneDreamer learns to generate unbounded 3D scenes from in-the-wild 2D image collections. Our method can synthesize diverse landscapes across different styles, with 3D consistency, well-defined depth, and free camera trajectory.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/f2nerf.png' width="200">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="https://totoro97.github.io/projects/f2-nerf/">
                <papertitle>F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories</papertitle>
              </a>
              <br>
              <a href="https://nearlyemptystring.com/about">Peng Wang*</a>,
              <a href="https://liuyuan-pal.github.io/">Yuan Liu*</a>,
              <strong>Zhaoxi Chen</strong>,
              <a href="https://lingjie0206.github.io/">Lingjie Liu</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>,
              <a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku">Taku Komura</a>,
              <a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>,
              <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a>
              <br>
              <em>CVPR</em>, 2023 &nbsp <font color="#ff6a5c"><strong>(Highlight)</strong></font>
              <br>
              [<a href="https://totoro97.github.io/projects/f2-nerf/">Project Page</a>]
							[<a href="https://arxiv.org/abs/2303.15951">Paper</a>]
							[<a href="https://github.com/totoro97/f2-nerf">Code</a>]
              <img src="https://img.shields.io/github/stars/totoro97/f2-nerf?style=social">
              <img src="https://img.shields.io/github/forks/totoro97/f2-nerf?style=social">
              <p></p>
              <p>F2-NeRF enables arbitrary input camera trajectories for novel view synthesis and only costs a few minutes for training.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/eva3d.gif' width="200">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="https://hongfz16.github.io/projects/EVA3D.html">
                <papertitle>EVA3D: Compositional 3D Human Generation from 2D Image Collections</papertitle>
              </a>
              <br>
              <a href="https://hongfz16.github.io/">Fangzhou Hong</a>,
              <strong>Zhaoxi Chen</strong>,
              <a href="https://github.com/NIRVANALAN">Yushi Lan</a>,
              <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&hl=zh-CN">Liang Pan</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>ICLR</em>, 2023 &nbsp <font color="#ff6a5c"><strong>(Spotlight)</strong></font>
              <br>
              [<a href="https://hongfz16.github.io/projects/EVA3D.html">Project Page</a>]
							[<a href="https://arxiv.org/abs/2210.04888">Paper</a>]
							[<a href="https://www.youtube.com/watch?v=JNV0FJ0aDWM">Video</a>]
							[<a href="https://github.com/hongfz16/EVA3D">Code</a>]
              <img src="https://img.shields.io/github/stars/hongfz16/EVA3D?style=social">
              <img src="https://img.shields.io/github/forks/hongfz16/EVA3D?style=social">
              <p></p>
              <p>EVA3D is a high-quality unconditional 3D human generative model that only requires 2D image collections for training.</p>
            </td>
          </tr>
          <tr onmouseout="text2light_stop()" onmouseover="text2light_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='text2light_video'><img src="images/text2light.gif" width="200"></div>
                <img src='images/text2light.jpg' width="200">
              </div>
              <script type="text/javascript">
                function text2light_start() {
                  document.getElementById('text2light_video').style.opacity = "1";
                }

                function text2light_stop() {
                  document.getElementById('text2light_video').style.opacity = "0";
                }
                text2light_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="projects/text2light/">
                <papertitle>Text2Light: Zero-Shot Text-Driven HDR Panorama Generation</papertitle>
              </a>
              <br>
              <strong>Zhaoxi Chen</strong>,
              <a href="https://wanggcong.github.io/">Guangcong Wang</a>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>TOG</em>, 2022 (Proc. SIGGRAPH Asia)
              <br>
              [<a href="projects/text2light/">Project Page</a>]
							[<a href="https://arxiv.org/abs/2209.09898">Paper</a>]
							[<a href="https://youtu.be/XDx6tOHigPE">Video</a>]
              [<a href="https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb">Colab</a>]
							[<a href="https://github.com/FrozenBurning/Text2Light">Code</a>]
              <img src="https://img.shields.io/github/stars/FrozenBurning/Text2Light?style=social">
              <img src="https://img.shields.io/github/forks/FrozenBurning/Text2Light?style=social">
              <p></p>
              <p>Text2Light can generate HDR panoramas in 4K+ resolution using free-form texts, without training on text-image pairs. The high-quality generated HDR panoramas can be directly applied to downstream tasks, e.g., light 3D scenes and immersive virtual reality.</p>
            </td>
          </tr>
          <tr onmouseout="relighting4d_stop()" onmouseover="relighting4d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='relighting4d_video'><img src="images/relighting.gif" width="200"></div>
                <img src='images/relighting.jpg' width="200">
              </div>
              <script type="text/javascript">
                function relighting4d_start() {
                  document.getElementById('relighting4d_video').style.opacity = "1";
                }

                function relighting4d_stop() {
                  document.getElementById('relighting4d_video').style.opacity = "0";
                }
                relighting4d_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="projects/relighting4d">
                <papertitle>Relighting4D: Neural Relightable Human from Videos</papertitle>
              </a>
              <br>
              <strong>Zhaoxi Chen</strong>,
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              [<a href="projects/relighting4d">Project Page</a>]
							[<a href="https://arxiv.org/abs/2207.07104">Paper</a>]
							[<a href="https://github.com/FrozenBurning/Relighting4D">Code</a>]
              <img src="https://img.shields.io/github/stars/FrozenBurning/Relighting4D?style=social">
              <img src="https://img.shields.io/github/forks/FrozenBurning/Relighting4D?style=social">
              <p></p>
              <p>Relighting4D takes only videos as input, decomposing them into geometry and reflectance in a self-supervised manner, which enables relighting of dynamic humans with free viewpoints by a physically based renderer.</p>
            </td>
          </tr>
          <tr onmouseout="adafocus_stop()" onmouseover="adafocus_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='adafocus_video'><img src="images/adafocus.gif" width="200"></div>
                <img src='images/adafocus.png' width="200">
              </div>
              <script type="text/javascript">
                function adafocus_start() {
                  document.getElementById('adafocus_video').style.opacity = "1";
                }

                function adafocus_stop() {
                  document.getElementById('adafocus_video').style.opacity = "0";
                }
                adafocus_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
							<a href="https://github.com/blackfeather-wang/AdaFocus">
                <papertitle>Adaptive Focus for Efficient Video Recognition</papertitle>
              </a>
              <br>
              <a href="https://www.rainforest-wang.cool/">Yulin Wang<strong>*</strong></a>,
              <strong>Zhaoxi Chen*</strong>,
              <a href="https://github.com/jianghaojun">Haojun Jiang</a>,
              <a href="https://scholar.google.com/citations?user=rw6vWdcAAAAJ&hl=en">Shiji Song</a>,
              <a href="https://yizenghan.top/">Yizeng Han</a>,
              <a href="http://www.gaohuang.net/">Gao Huang</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp <font color="#ff6a5c"><strong>(Oral Presentation)</strong></font>
              <br>
							[<a href="https://arxiv.org/abs/2105.03245">Paper</a>]
							[<a href="https://github.com/blackfeather-wang/AdaFocus">Code</a>]
              <img src="https://img.shields.io/github/stars/blackfeather-wang/AdaFocus?style=social">
              <img src="https://img.shields.io/github/forks/blackfeather-wang/AdaFocus?style=social">
              <p></p>
              <p>In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is significantly more efficient than the competitive baselines.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src="images/ts-adv.jpg" alt="understanding" width="200" style="border-style: none">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.13170">
                <papertitle>Understanding Robustness in Teacher-Student Setting: A New Perspective</papertitle>
              </a>
              <br>
              <a href="https://lucas110550.github.io/about/">Zhuolin Yang<strong>*</strong></a>,
              <strong>Zhaoxi Chen</strong>,
              Tiffany Cai,
              <a href="https://jungyhuk.github.io/">Xinyun Chen</a>,
              <a href="https://aisecure.github.io/">Bo Li</a>,
              <a href="https://yuandong-tian.com/">Yuandong Tian<strong>*</strong></a>
              <br>
              <em>AISTATS</em>, 2021
              <br>
              [<a href="https://arxiv.org/abs/2102.13170">Paper</a>]
              <p>In the case of low-rank input data, we show that student specialization still happens within the input subspace, but the teacher and student nodes could differ wildly out of the data subspace, which we conjecture leads to adversarial examples.</p>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%">
              <a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>, Nanyang Technological University <br>
              Ph.D. student supervised by Prof. <a href="https://liuziwei7.github.io">Ziwei Liu</a>
            </td>
            <td width="25%">Aug. 2021 - Now</td>
          </tr>
          <tr>
            <td width="75%">
              <a href="https://www.bnrist.tsinghua.edu.cn/bnristen/">BNRist</a>, Tsinghua University <br>
              Undergraduate thesis supervised by Prof. <a href="http://www.gaohuang.net/">Gao Huang</a>
            </td>
            <td width="25%">Dec. 2020 - Jul. 2021</td>
          </tr>
          <tr>
            <td width="75%">
              <a href="https://aisecure.github.io/">Secure Learning Lab</a>, University of Illinois at Urbana-Champaign <br>
              Visiting research intern advised by Prof. <a href="https://aisecure.github.io/">Bo Li</a>
            </td>
            <td width="25%">May. 2020 - Oct. 2020</td>
          </tr>
          <tr>
            <td width="75%">
              <a href="https://www.modlabupenn.org/">GRASP</a>, University of Pennsylvania <br>
              Intern advised by Prof. <a href="https://www.seas.upenn.edu/directory/profile.php?ID=107">Mark Yim</a>
            </td>
            <td width="25%">Jun. 2020 - Sep. 2020</td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <p><a href="https://research.facebook.com/blog/2023/4/announcing-the-2023-meta-research-phd-fellowship-award-winners/">Meta Research PhD Fellowship</a>, Finalist, 2023</p>
            <p><a href="https://aisingapore.org/research/aisg-phd-fellowship-programme/">AISG PhD Fellowship</a>, 2021</p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Services</heading>
          <p>Conference Reviewer: CVPR, ICCV, SIGGRAPH, NeurIPS</p>
          <p>Journal Reviewer: TOG, IJCV</p>
        </td>
      </tr>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Misc</heading>
          <p>I'm a fanatical music lover, with skills in saxophone, piano, and guitar.</p>
        </td>
      </tr>
    </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <td style="text-align: right;"> Website template credits to <a href="http://jonbarron.info">Jon Barron</a>. </td>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
